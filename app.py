import streamlit as st
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import pandas as pd
import io
import time
import visuals  # Ensure visuals.py exists in your repo

# --- CONFIGURATION ---
st.set_page_config(page_title="Agentic Readiness Auditor Pro", page_icon="ü§ñ", layout="wide")

# --- SESSION STATE INITIALIZATION ---
if 'audit_data' not in st.session_state:
    st.session_state['audit_data'] = None
if 'recs' not in st.session_state:
    st.session_state['recs'] = None
if 'ai_summary' not in st.session_state:
    st.session_state['ai_summary'] = None
if 'current_url' not in st.session_state:
    st.session_state['current_url'] = ""

# --- FUNCTIONS ---

def detect_tech_stack(soup, headers):
    """Detects if the site is WP, Shopify, Next.js, etc."""
    stack = []
    html = str(soup)
    
    if "wp-content" in html or "WordPress" in str(soup.find("meta", attrs={"name": "generator"})):
        stack.append("WordPress")
    if "cdn.shopify.com" in html or "Shopify" in html:
        stack.append("Shopify")
    if "woocommerce" in html:
        stack.append("WooCommerce")
    if "__NEXT_DATA__" in html:
        stack.append("Next.js")
    if "Wix" in html:
        stack.append("Wix")
    if "Squarespace" in html:
        stack.append("Squarespace")
        
    return ", ".join(stack) if stack else "Custom/Unknown Stack"

def check_security_gates(url):
    domain = url.rstrip('/')
    gates = {}
    
    # 1. Robots.txt
    try:
        r = requests.get(f"{domain}/robots.txt", timeout=3)
        if r.status_code == 200:
            gates['robots.txt'] = "Found"
            if "GPTBot" in r.text and "Disallow" in r.text:
                gates['ai_access'] = "BLOCKED (Critical)"
            else:
                gates['ai_access'] = "Allowed"
        else:
            gates['robots.txt'] = "Missing"
            gates['ai_access'] = "Uncontrolled"
    except:
        gates['robots.txt'] = "Error"
        gates['ai_access'] = "Unknown"

    # 2. Sitemap
    try:
        s_urls = [f"{domain}/sitemap.xml", f"{domain}/sitemaps.xml", f"{domain}/sitemap_index.xml", f"{domain}/wp-sitemap.xml"]
        found_sitemap = False
        for s_url in s_urls:
            try:
                if requests.get(s_url, timeout=2).status_code == 200:
                    gates['sitemap.xml'] = f"Found ({s_url.split('/')[-1]})"
                    found_sitemap = True
                    break
            except:
                continue
        if not found_sitemap:
            gates['sitemap.xml'] = "Missing"
    except:
        gates['sitemap.xml'] = "Error checking"

    # 3. ai.txt
    try:
        if requests.get(f"{domain}/ai.txt", timeout=3).status_code == 200:
            gates['ai.txt'] = "Found"
        else:
            gates['ai.txt'] = "Missing"
    except:
        gates['ai.txt'] = "Error"
        
    return gates

def generate_recommendations(audit_data):
    recs = []
    if "BLOCKED" in audit_data['gates']['ai_access']:
        recs.append("CRITICAL: Update robots.txt to whitelist 'GPTBot' and 'Google-Extended'.")
    if audit_data['schema_count'] == 0:
        recs.append("HIGH PRIORITY: Implement JSON-LD Schema. Agents cannot understand your content structure.")
    if "Missing" in audit_data['gates']['ai.txt']:
        recs.append("OPTIMIZATION: Create an 'ai.txt' file to explicitly grant permission to AI models.")
    return recs

def generate_fallback_summary(audit_data):
    """FAIL-SAFE: Writes a report manually if AI fails."""
    is_shop = "Shopify" in audit_data['stack'] or "WooCommerce" in audit_data['stack']
    
    if is_shop:
        summary = f"""
### 1. Executive Summary
This **E-commerce** site using {audit_data['stack']} is accessible but lacks key Agentic protocols. The absence of an **ai.txt** file means AI buyers have no clear rules. Without specific permissions, automated **transactions** and product discovery may be unreliable.

### 2. Business Impact Analysis
* **Missing ai.txt:** Agents cannot verify permissions, leading to abandoned **autonomous carts**.
* **Schema Gaps:** Products may be invisible to price-comparison bots, causing lost **sales**.
* **Risk:** Competitors with optimized 'Agent Ready' sites will capture the AI-driven market share.
"""
    else:
        summary = f"""
### 1. Executive Summary
This site runs on {audit_data['stack']} and lacks essential **Agentic** standards. The absence of an **ai.txt** file prevents controlled **content retrieval** by AI systems. This limits the site's ability to be accurately cited by LLMs for **lead generation**.

### 2. Business Impact Analysis
* **Missing ai.txt:** AI agents may scrape irrelevant data or ignore the site, reducing **brand visibility**.
* **Schema Gaps:** Services cannot be machine-read, leading to **hallucinated** answers about your business.
* **Risk:** Reduced organic traffic from AI-powered search engines like SearchGPT.
"""
    return summary + "\n\n*(Note: Generated by Fallback Logic due to AI Service Congestion)*"

def perform_audit(url, api_key):
    # OPENROUTER CONNECTION (Matches requirements.txt)
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=api_key,
    )
    
    # THE TANK LIST (Reliable Free Models)
    models = [
        "google/gemini-2.0-flash-exp:free",
        "meta-llama/llama-3.2-11b-vision-instruct:free",
        "microsoft/phi-3-medium-128k-instruct:free",
        "huggingfaceh4/zephyr-7b-beta:free"
    ]
    
    status = st.empty()
    status.text("üîç Scanning website structure...")
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; AgenticAuditor/1.0)'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Gather Context
        title = soup.title.string if soup.title else "No Title"
        body = soup.body.get_text(separator=' ', strip=True)[:1000] if soup.body else ""
        context = f"Title: {title}\nContent: {body}"
        
        # Run Checks
        stack = detect_tech_stack(soup, response.headers)
        gates = check_security_gates(url)
        schemas = soup.find_all('script', type='application/ld+json')
        
        # Manifest Check
        domain = url.rstrip('/')
        manifest = "Missing"
        try:
            if requests.get(f"{domain}/manifest.json", timeout=2).status_code == 200:
                manifest = "Found"
            elif soup.find("link", rel="manifest"):
                manifest = "Found (Linked)"
        except:
            pass

        audit_data = {
            "url": url,
            "stack": stack,
            "gates": gates,
            "schema_count": len(schemas),
            "schema_sample": "",
            "manifest": manifest
        }
        recs = generate_recommendations(audit_data)
        
        # AI Generation
        status.text("ü§ñ Generative AI is writing the report...")
        prompt = f"""
        Analyze this website audit for 'Agentic Readiness'.
        URL: {url} | Stack: {stack} | Gates: {gates} | Schema: {len(schemas)} | Manifest: {manifest}
        CONTEXT: {context}
        
        TASK:
        1. Identify Business Type (Store vs Service).
        2. Write Executive Summary (3 sentences, use **Bold**).
        3. Write Business Impact (3 bullets, <25 words each).
        Strict Markdown.
        """
        
        ai_summary = None
        for model in models:
            try:
                completion = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}]
                )
                ai_summary = completion.choices[0].message.content
                break # Success
            except:
                continue # Try next
        
        # FAIL-SAFE
        if not ai_summary:
            ai_summary = generate_fallback_summary(audit_data)
            
        status.empty()
        return audit_data, recs, ai_summary

    except Exception as e:
        st.error(f"Connection Error: {str(e)}")
        return None, None, None

# --- UI LAYOUT ---

st.sidebar.title("üïµÔ∏è‚Äç‚ôÇÔ∏è Audit Controls")
# NOTE: The label explicitly asks for OpenRouter Key now
api_key = st.sidebar.text_input("OpenRouter API Key", type="password")

st.title("ü§ñ Agentic Readiness Auditor Pro")
st.markdown("### The Standard for Future Commerce")
st.info("Check if your client's website is ready for the **Agent Economy** (Mastercard/Visa Agents, ChatGPT, Gemini).")

# Use session_state for value to prevent 'sticky' input issues
if 'current_url' not in st.session_state:
    st.session_state['current_url'] = ""

url_input = st.text_input("Enter Client Website URL", value=st.session_state['current_url'], placeholder="https://www.example-hotel.com")

if st.button("üöÄ Run Full Audit"):
    if not api_key or not url_input:
        st.error("Please provide both API Key and URL.")
    else:
        # Save current URL to session state
        st.session_state['current_url'] = url_input
        
        # Run Audit
        data, recommendations, summary = perform_audit(url_input, api_key)
        
        if data:
            st.session_state['audit_data'] = data
            st.session_state['recs'] = recommendations
            st.session_state['ai_summary'] = summary

# --- DISPLAY RESULTS (Outside the button logic so it persists) ---
if st.session_state['audit_data']:
    st.success("‚úÖ Audit Complete!")
    
    # 1. Graphical Dashboard
    visuals.display_dashboard(st.session_state['audit_data'])

    st.divider()  
    
    # 2. Text Report
    st.subheader("üìù Executive Summary")
    st.write(st.session_state['ai_summary'])
    
    st.subheader("üîß Priority Recommendations")
    for rec in st.session_state['recs']:
        st.warning(rec)
        
    # 3. Excel Report Generation
    report_dict = {
        "Metric": ["Target URL", "Tech Stack", "Robots.txt Status", "AI.txt Status", "Schema Objects", "AI Manifest"],
        "Status": [
            st.session_state['audit_data']['url'],
            st.session_state['audit_data']['stack'],
            st.session_state['audit_data']['gates']['robots.txt'],
            st.session_state['audit_data']['gates']['ai.txt'],
            f"{st.session_state['audit_data']['schema_count']} found",
            st.session_state['audit_data']['manifest']
        ]
    }
    df_report = pd.DataFrame(report_dict)
    
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        df_report.to_excel(writer, sheet_name='Audit Summary', index=False)
        df_recs = pd.DataFrame(st.session_state['recs'], columns=["Actionable Recommendations"])
        df_recs.to_excel(writer, sheet_name='Action Plan', index=False)
        
    # 4. Buttons (Download & New Audit)
    col1, col2 = st.columns(2)
    
    with col1:
        st.download_button(
            label="üì• Download Excel Report",
            data=buffer,
            file_name=f"Agentic_Audit_{int(time.time())}.xlsx",
            mime="application/vnd.ms-excel"
        )
        
    with col2:
        if st.button("üîÑ Start New Audit"):
            # Clear Session State
            st.session_state['audit_data'] = None
            st.session_state['recs'] = None
            st.session_state['ai_summary'] = None
            st.session_state['current_url'] = ""
            st.rerun()
