import streamlit as st
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import pandas as pd
import io
import time
import visuals

# --- CONFIGURATION ---
st.set_page_config(page_title="Agentic Readiness Auditor Pro", page_icon="üïµÔ∏è‚Äç‚ôÇÔ∏è", layout="wide")

# --- SESSION STATE INITIALIZATION ---
if 'audit_data' not in st.session_state:
    st.session_state['audit_data'] = None
if 'recs' not in st.session_state:
    st.session_state['recs'] = None
if 'ai_summary' not in st.session_state:
    st.session_state['ai_summary'] = None
if 'current_url' not in st.session_state:
    st.session_state['current_url'] = ""

# --- FUNCTIONS ---

def detect_tech_stack(soup, headers):
    """Detects if the site is WP, Shopify, Next.js, etc."""
    stack = []
    html = str(soup)
    
    if "wp-content" in html or "WordPress" in str(soup.find("meta", attrs={"name": "generator"})):
        stack.append("WordPress")
    if "cdn.shopify.com" in html or "Shopify" in html:
        stack.append("Shopify")
    if "woocommerce" in html:
        stack.append("WooCommerce")
    if "__NEXT_DATA__" in html:
        stack.append("Next.js")
    if "Wix" in html:
        stack.append("Wix")
    if "Squarespace" in html:
        stack.append("Squarespace")
        
    return ", ".join(stack) if stack else "Custom/Unknown Stack"

def check_security_gates(url):
    domain = url.rstrip('/')
    gates = {}
    
    # 1. Robots.txt
    try:
        r = requests.get(f"{domain}/robots.txt", timeout=3)
        if r.status_code == 200:
            gates['robots.txt'] = "Found"
            if "GPTBot" in r.text and "Disallow" in r.text:
                gates['ai_access'] = "BLOCKED (Critical)"
            else:
                gates['ai_access'] = "Allowed"
        else:
            gates['robots.txt'] = "Missing"
            gates['ai_access'] = "Uncontrolled"
    except:
        gates['robots.txt'] = "Error"
        gates['ai_access'] = "Unknown"

    # 2. Sitemap
    try:
        s_urls = [f"{domain}/sitemap.xml", f"{domain}/sitemaps.xml", f"{domain}/sitemap_index.xml", f"{domain}/wp-sitemap.xml"]
        found_sitemap = False
        for s_url in s_urls:
            try:
                if requests.get(s_url, timeout=2).status_code == 200:
                    gates['sitemap.xml'] = f"Found ({s_url.split('/')[-1]})"
                    found_sitemap = True
                    break
            except:
                continue
        if not found_sitemap:
            gates['sitemap.xml'] = "Missing"
    except:
        gates['sitemap.xml'] = "Error checking"

    # 3. ai.txt
    try:
        if requests.get(f"{domain}/ai.txt", timeout=3).status_code == 200:
            gates['ai.txt'] = "Found"
        else:
            gates['ai.txt'] = "Missing"
    except:
        gates['ai.txt'] = "Error"
        
    return gates

def generate_recommendations(audit_data):
    recs = []
    if "BLOCKED" in audit_data['gates']['ai_access']:
        recs.append("CRITICAL: Update robots.txt to whitelist 'GPTBot' and 'Google-Extended'.")
    if audit_data['schema_count'] == 0:
        recs.append("HIGH PRIORITY: Implement JSON-LD Schema. Agents cannot understand your content structure.")
    if "Missing" in audit_data['gates']['ai.txt']:
        recs.append("OPTIMIZATION: Create an 'ai.txt' file to explicitly grant permission to AI models.")
    return recs

def generate_fallback_summary(audit_data):
    """FAIL-SAFE: Writes a report manually if AI fails."""
    is_shop = "Shopify" in audit_data['stack'] or "WooCommerce" in audit_data['stack']
    
    if is_shop:
        summary = f"""
### 1. Executive Summary
This **E-commerce** site using {audit_data['stack']} is accessible but lacks key Agentic protocols. The absence of an **ai.txt** file means AI buyers have no clear rules. Without specific permissions, automated **transactions** and product discovery may be unreliable.

### 2. Business Impact Analysis
* **Missing ai.txt:** Agents cannot verify permissions, leading to abandoned **autonomous carts**.
* **Schema Gaps:** Products may be invisible to price-comparison bots, causing lost **sales**.
* **Risk:** Competitors with optimized 'Agent Ready' sites will capture the AI-driven market share.
"""
    else:
        summary = f"""
### 1. Executive Summary
This site runs on {audit_data['stack']} and lacks essential **Agentic** standards. The absence of an **ai.txt** file prevents controlled **content retrieval** by AI systems. This limits the site's ability to be accurately cited by LLMs for **lead generation**.

### 2. Business Impact Analysis
* **Missing ai.txt:** AI agents may scrape irrelevant data or ignore the site, reducing **brand visibility**.
* **Schema Gaps:** Services cannot be machine-read, leading to **hallucinated** answers about your business.
* **Risk:** Reduced organic traffic from AI-powered search engines like SearchGPT.
"""
    return summary + "\n\n*(Note: Generated by Fallback Logic due to AI Service Congestion)*"

def perform_audit(url, api_key):
    # OPENROUTER CONNECTION (Requires 'openai' library, NOT 'google.generativeai')
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=api_key,
    )
    
    # THE TANK LIST (Reliable Free Models)
    # I have updated these to the latest stable Free models on OpenRouter
    models = [
        "google/gemini-2.0-flash-exp:free",        # Primary
        "meta-llama/llama-3.2-11b-vision-instruct:free", # Robust Backup
        "microsoft/phi-3-medium-128k-instruct:free",    # Reliable Backup
        "huggingfaceh4/zephyr-7b-beta:free"             # Last Resort
    ]
    
    status = st.empty()
    status.text("üîç Scanning website structure...")
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; AgenticAuditor/1.0)'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Gather Context
        title = soup.title.string if soup.title else "No Title"
        body = soup.body.get_text(separator=' ', strip=True)[:1000] if soup.body else ""
        context = f"Title: {title}\nContent: {body}"
        
        # Run Checks
        stack = detect_tech_stack(soup, response.headers)
        gates = check_security_gates(url)
        schemas = soup.find_all('script', type='application/ld+json')
        
        # Manifest Check
        domain = url.rstrip('/')
        manifest = "Missing"
        try:
            if requests.get(f"{domain}/manifest.json", timeout=2).status_code == 200:
                manifest = "Found"
            elif soup.find("link", rel="manifest"):
                manifest = "Found (Linked)"
        except:
            pass

        audit_data = {
            "url": url,
            "stack": stack,
            "gates": gates,
            "schema_count": len(schemas),
            "schema_sample": "",
            "manifest": manifest
        }
        recs = generate_recommendations(audit_data)
        
        # 5. Gemini Analysis
        status.text("Generative AI is reading the content to identify business type...")
        prompt = f"""
        You are a Senior Technical Consultant. Analyze this website for 'Agentic Readiness'.
        
        TARGET DATA:
        - URL: {url}
        - Tech Stack: {stack}
        - Security Gates: {gates}
        - Schema Found: {len(schemas)} items.
        - Manifest Status: {manifest_status}
        
        WEBSITE CONTEXT:
        {site_context}
        
        YOUR TASK:
        1. Detect the Business Type (E-commerce, SaaS, B2B, Blog, etc.) based on the context.
        
        2. GENERATE A REPORT IN STRICT MARKDOWN FORMAT:
        
        ### 1. Executive Summary
        - Write exactly 3 short, punchy sentences.
        - Use **Bold** for key terms.
        - Tailor the language to the business type.
            
        ### 2. Business Impact Analysis
        - Provide in Bullet Points with brief of your technical observations.
        - Each bullet must start with a **Bold Issue**.
        - Keep each bullet upto 38 words length.
        
        Do NOT write paragraphs too long. Delivering messages that are easy to understand.
        """
        
           
        ai_summary = None
        for model in models:
            try:
                # Try to generate content
                completion = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}]
                )
                ai_summary = completion.choices[0].message.content
                if ai_summary: break # If we got a response, exit the loop
            except:
                continue # If error (429/404), try the next model
        
        # FAIL-SAFE: If all AI models failed, use the manual generator
        if not ai_summary:
            ai_summary = generate_fallback_summary(audit_data)
            
        status.empty()
        return audit_data, recs, ai_summary

    except Exception as e:
        st.error(f"Connection Error: {str(e)}")
        return None, None, None

# --- UI LAYOUT ---

st.sidebar.title("üïµÔ∏è‚Äç‚ôÇÔ∏è Audit Controls")
# Option to enter key or use hardcoded one
user_input_key = st.sidebar.text_input("OpenRouter API Key", type="password", help="Leave empty to use system key")

# HARDCODED KEY LOGIC (If you want to hardcode it, paste it below)
if user_input_key:
    api_key = user_input_key
else:
    api_key = "" # PASTE YOUR OPENROUTER KEY HERE IF YOU WANT

st.title("ü§ñ Agentic Readiness Auditor Pro")
st.markdown("### The Standard for Future Commerce")
st.info("Check if your client's website is ready for the **Agent Economy** (Mastercard/Visa Agents, ChatGPT, Gemini).")

# Main Input
if 'current_url' not in st.session_state:
    st.session_state['current_url'] = ""

# THE FORM (Fixed "Enter Key" Logic + URL Fixer)
with st.form(key='audit_form'):
    url_input = st.text_input("Enter Client Website URL", placeholder="example.com")
    submit_button = st.form_submit_button("üöÄ Run Full Audit")

# LOGIC HANDLING
if submit_button:
    if not api_key:
        st.error("Please provide an API Key in the sidebar.")
    elif not url_input:
        st.error("Please provide a URL.")
    else:
        # --- URL FIXER ---
        if not url_input.startswith(("http://", "https://")):
            url_input = "https://" + url_input
            
        # 1. Clear previous data immediately
        st.session_state['audit_data'] = None
        st.session_state['recs'] = None
        st.session_state['ai_summary'] = None
        
        # 2. Save URL
        st.session_state['current_url'] = url_input
        
        # 3. Run Audit
        data, recommendations, summary = perform_audit(url_input, api_key)
        
        if data:
            st.session_state['audit_data'] = data
            st.session_state['recs'] = recommendations
            st.session_state['ai_summary'] = summary

# --- REPORT CONTAINER (Fixed Layout Order) ---
report_view = st.empty()

# --- DISPLAY RESULTS ---
if st.session_state['audit_data']:
    with report_view.container():
        st.success("‚úÖ Audit Complete!")
        
        # 1. Graphical Dashboard
        visuals.display_dashboard(st.session_state['audit_data'])

        st.divider()  
        
        # 2. Text Report
        st.subheader("üìù Executive Summary")
        st.write(st.session_state['ai_summary'])
        
        st.subheader("üîß Priority Recommendations")
        for rec in st.session_state['recs']:
            st.warning(rec)
            
        # 3. Excel Report Generation
        report_dict = {
            "Metric": ["Target URL", "Tech Stack", "Robots.txt Status", "AI.txt Status", "Schema Objects", "AI Manifest"],
            "Status": [
                st.session_state['audit_data']['url'],
                st.session_state['audit_data']['stack'],
                st.session_state['audit_data']['gates']['robots.txt'],
                st.session_state['audit_data']['gates']['ai.txt'],
                f"{st.session_state['audit_data']['schema_count']} found",
                st.session_state['audit_data']['manifest']
            ]
        }
        df_report = pd.DataFrame(report_dict)
        
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_report.to_excel(writer, sheet_name='Audit Summary', index=False)
            df_recs = pd.DataFrame(st.session_state['recs'], columns=["Actionable Recommendations"])
            df_recs.to_excel(writer, sheet_name='Action Plan', index=False)
            
        # 4. Buttons (Download & New Audit)
        col1, col2 = st.columns(2)
        
        with col1:
            st.download_button(
                label="üì• Download Excel Report",
                data=buffer,
                file_name=f"Agentic_Audit_{int(time.time())}.xlsx",
                mime="application/vnd.ms-excel"
            )
            
        with col2:
            if st.button("üîÑ Start New Audit"):
                st.session_state['audit_data'] = None
                st.session_state['recs'] = None
                st.session_state['ai_summary'] = None
                st.session_state['current_url'] = ""
                st.rerun()
